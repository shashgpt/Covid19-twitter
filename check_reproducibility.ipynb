{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bf8e704-5b42-4d30-bbea-66ebcd6f1c58",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb45c2d8-b323-4790-8ce1-1dcaa6d82b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emojis</th>\n",
       "      <th>emoji_names</th>\n",
       "      <th>emotion_scores</th>\n",
       "      <th>agg_emotion_score</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>vader_score_sentence</th>\n",
       "      <th>vader_score_clause_A</th>\n",
       "      <th>vader_score_clause_B</th>\n",
       "      <th>vader_sentiment_sentence</th>\n",
       "      <th>vader_sentiment_clause_A</th>\n",
       "      <th>vader_sentiment_clause_B</th>\n",
       "      <th>rule_structure</th>\n",
       "      <th>rule_label</th>\n",
       "      <th>contrast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1292641373758328839</td>\n",
       "      <td>['😢', '😢']</td>\n",
       "      <td>['crying face', 'crying face']</td>\n",
       "      <td>[[-0.25, -0.36, -0.5, 0.0, -1.0, 0.11], [-0.25...</td>\n",
       "      <td>-4.00</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.249, 'neu': 0.751, 'pos': 0.0, 'comp...</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>negative</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>no_structure</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1377497788460720129</td>\n",
       "      <td>['😂', '😂', '😂', '😂']</td>\n",
       "      <td>['face with tears of joy', 'face with tears of...</td>\n",
       "      <td>[[0.0, -0.06, -0.06, 0.94, 0.0, 0.22], [0.0, -...</td>\n",
       "      <td>4.16</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.021, 'neu': 0.909, 'pos': 0.07, 'com...</td>\n",
       "      <td>{'neg': 0.036, 'neu': 0.909, 'pos': 0.056, 'co...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.9, 'pos': 0.1, 'compound...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>A-but-B</td>\n",
       "      <td>A-but-B</td>\n",
       "      <td>no_contrast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1318434591322087427</td>\n",
       "      <td>['💔', '💔', '💔']</td>\n",
       "      <td>['broken heart', 'broken heart', 'broken heart']</td>\n",
       "      <td>[[-0.39, -0.33, -0.14, 0.0, -0.94, 0.17], [-0....</td>\n",
       "      <td>-4.89</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.176, 'neu': 0.824, 'pos': 0.0, 'comp...</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>negative</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>no_structure</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1303627865796222976</td>\n",
       "      <td>['😠', '😠', '😠', '😠', '😠']</td>\n",
       "      <td>['angry face', 'angry face', 'angry face', 'an...</td>\n",
       "      <td>[[-1.0, -0.56, -0.17, 0.0, -0.25, 0.08], [-1.0...</td>\n",
       "      <td>-9.50</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.117, 'neu': 0.883, 'pos': 0.0, 'comp...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>{'neg': 0.302, 'neu': 0.698, 'pos': 0.0, 'comp...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>A-yet-B</td>\n",
       "      <td>A-yet-B</td>\n",
       "      <td>contrast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1361789228426932227</td>\n",
       "      <td>['🎉', '🎉', '🎉']</td>\n",
       "      <td>['party popper', 'party popper', 'party popper']</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.92, 0.0, 0.33], [0.0, 0.0, ...</td>\n",
       "      <td>3.75</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.686, 'pos': 0.314, 'comp...</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>no_structure</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109914</th>\n",
       "      <td>1304993476564590592</td>\n",
       "      <td>['😡', '😡', '😡']</td>\n",
       "      <td>['pouting face', 'pouting face', 'pouting face']</td>\n",
       "      <td>[[-1.0, -0.56, -0.11, 0.0, -0.36, 0.06], [-1.0...</td>\n",
       "      <td>-5.91</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.192, 'neu': 0.808, 'pos': 0.0, 'comp...</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>negative</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>no_structure</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109915</th>\n",
       "      <td>1389249469850394627</td>\n",
       "      <td>['💐', '💐', '💐', '💐', '👍']</td>\n",
       "      <td>['bouquet', 'bouquet', 'bouquet', 'bouquet', '...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.69, -0.11, 0.58], [0.0, 0.0...</td>\n",
       "      <td>5.19</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.291, 'pos': 0.709, 'comp...</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>no_structure</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109916</th>\n",
       "      <td>1322678255128903683</td>\n",
       "      <td>['😢', '😢']</td>\n",
       "      <td>['crying face', 'crying face']</td>\n",
       "      <td>[[-0.25, -0.36, -0.5, 0.0, -1.0, 0.11], [-0.25...</td>\n",
       "      <td>-4.00</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.106, 'neu': 0.846, 'pos': 0.049, 'co...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.822, 'pos': 0.178, 'comp...</td>\n",
       "      <td>{'neg': 0.133, 'neu': 0.867, 'pos': 0.0, 'comp...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>A-but-B</td>\n",
       "      <td>A-but-B</td>\n",
       "      <td>contrast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109917</th>\n",
       "      <td>1361083777724915712</td>\n",
       "      <td>['🙏', '🙏', '🙏', '🙏', '🙏', '🙏', '🙏', '🙏', '🙏', ...</td>\n",
       "      <td>['folded hands', 'folded hands', 'folded hands...</td>\n",
       "      <td>[[-0.06, 0.0, -0.11, 0.25, -0.11, 0.58], [-0.0...</td>\n",
       "      <td>5.50</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.79, 'pos': 0.21, 'compou...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.748, 'pos': 0.252, 'comp...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>A-while-B</td>\n",
       "      <td>A-while-B</td>\n",
       "      <td>contrast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109918</th>\n",
       "      <td>1286590732728377344</td>\n",
       "      <td>['🌺', '💖', '✨', '💫']</td>\n",
       "      <td>['hibiscus', 'sparkling heart', 'sparkles', 'd...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.39, 0.0, 0.19], [0.0, 0.0, ...</td>\n",
       "      <td>3.27</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.752, 'pos': 0.248, 'comp...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.88, 'pos': 0.12, 'compou...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.288, 'pos': 0.712, 'comp...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>A-yet-B</td>\n",
       "      <td>A-yet-B</td>\n",
       "      <td>no_contrast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109919 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   tweet_id  \\\n",
       "0       1292641373758328839   \n",
       "1       1377497788460720129   \n",
       "2       1318434591322087427   \n",
       "3       1303627865796222976   \n",
       "4       1361789228426932227   \n",
       "...                     ...   \n",
       "109914  1304993476564590592   \n",
       "109915  1389249469850394627   \n",
       "109916  1322678255128903683   \n",
       "109917  1361083777724915712   \n",
       "109918  1286590732728377344   \n",
       "\n",
       "                                                   emojis  \\\n",
       "0                                              ['😢', '😢']   \n",
       "1                                    ['😂', '😂', '😂', '😂']   \n",
       "2                                         ['💔', '💔', '💔']   \n",
       "3                               ['😠', '😠', '😠', '😠', '😠']   \n",
       "4                                         ['🎉', '🎉', '🎉']   \n",
       "...                                                   ...   \n",
       "109914                                    ['😡', '😡', '😡']   \n",
       "109915                          ['💐', '💐', '💐', '💐', '👍']   \n",
       "109916                                         ['😢', '😢']   \n",
       "109917  ['🙏', '🙏', '🙏', '🙏', '🙏', '🙏', '🙏', '🙏', '🙏', ...   \n",
       "109918                               ['🌺', '💖', '✨', '💫']   \n",
       "\n",
       "                                              emoji_names  \\\n",
       "0                          ['crying face', 'crying face']   \n",
       "1       ['face with tears of joy', 'face with tears of...   \n",
       "2        ['broken heart', 'broken heart', 'broken heart']   \n",
       "3       ['angry face', 'angry face', 'angry face', 'an...   \n",
       "4        ['party popper', 'party popper', 'party popper']   \n",
       "...                                                   ...   \n",
       "109914   ['pouting face', 'pouting face', 'pouting face']   \n",
       "109915  ['bouquet', 'bouquet', 'bouquet', 'bouquet', '...   \n",
       "109916                     ['crying face', 'crying face']   \n",
       "109917  ['folded hands', 'folded hands', 'folded hands...   \n",
       "109918  ['hibiscus', 'sparkling heart', 'sparkles', 'd...   \n",
       "\n",
       "                                           emotion_scores  agg_emotion_score  \\\n",
       "0       [[-0.25, -0.36, -0.5, 0.0, -1.0, 0.11], [-0.25...              -4.00   \n",
       "1       [[0.0, -0.06, -0.06, 0.94, 0.0, 0.22], [0.0, -...               4.16   \n",
       "2       [[-0.39, -0.33, -0.14, 0.0, -0.94, 0.17], [-0....              -4.89   \n",
       "3       [[-1.0, -0.56, -0.17, 0.0, -0.25, 0.08], [-1.0...              -9.50   \n",
       "4       [[0.0, 0.0, 0.0, 0.92, 0.0, 0.33], [0.0, 0.0, ...               3.75   \n",
       "...                                                   ...                ...   \n",
       "109914  [[-1.0, -0.56, -0.11, 0.0, -0.36, 0.06], [-1.0...              -5.91   \n",
       "109915  [[0.0, 0.0, 0.0, 0.69, -0.11, 0.58], [0.0, 0.0...               5.19   \n",
       "109916  [[-0.25, -0.36, -0.5, 0.0, -1.0, 0.11], [-0.25...              -4.00   \n",
       "109917  [[-0.06, 0.0, -0.11, 0.25, -0.11, 0.58], [-0.0...               5.50   \n",
       "109918  [[0.0, 0.0, 0.0, 0.39, 0.0, 0.19], [0.0, 0.0, ...               3.27   \n",
       "\n",
       "       sentiment_label                               vader_score_sentence  \\\n",
       "0             negative  {'neg': 0.249, 'neu': 0.751, 'pos': 0.0, 'comp...   \n",
       "1             positive  {'neg': 0.021, 'neu': 0.909, 'pos': 0.07, 'com...   \n",
       "2             negative  {'neg': 0.176, 'neu': 0.824, 'pos': 0.0, 'comp...   \n",
       "3             negative  {'neg': 0.117, 'neu': 0.883, 'pos': 0.0, 'comp...   \n",
       "4             positive  {'neg': 0.0, 'neu': 0.686, 'pos': 0.314, 'comp...   \n",
       "...                ...                                                ...   \n",
       "109914        negative  {'neg': 0.192, 'neu': 0.808, 'pos': 0.0, 'comp...   \n",
       "109915        positive  {'neg': 0.0, 'neu': 0.291, 'pos': 0.709, 'comp...   \n",
       "109916        negative  {'neg': 0.106, 'neu': 0.846, 'pos': 0.049, 'co...   \n",
       "109917        positive  {'neg': 0.0, 'neu': 0.79, 'pos': 0.21, 'compou...   \n",
       "109918        positive  {'neg': 0.0, 'neu': 0.752, 'pos': 0.248, 'comp...   \n",
       "\n",
       "                                     vader_score_clause_A  \\\n",
       "0                                          not_applicable   \n",
       "1       {'neg': 0.036, 'neu': 0.909, 'pos': 0.056, 'co...   \n",
       "2                                          not_applicable   \n",
       "3       {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "4                                          not_applicable   \n",
       "...                                                   ...   \n",
       "109914                                     not_applicable   \n",
       "109915                                     not_applicable   \n",
       "109916  {'neg': 0.0, 'neu': 0.822, 'pos': 0.178, 'comp...   \n",
       "109917  {'neg': 0.0, 'neu': 0.748, 'pos': 0.252, 'comp...   \n",
       "109918  {'neg': 0.0, 'neu': 0.88, 'pos': 0.12, 'compou...   \n",
       "\n",
       "                                     vader_score_clause_B  \\\n",
       "0                                          not_applicable   \n",
       "1       {'neg': 0.0, 'neu': 0.9, 'pos': 0.1, 'compound...   \n",
       "2                                          not_applicable   \n",
       "3       {'neg': 0.302, 'neu': 0.698, 'pos': 0.0, 'comp...   \n",
       "4                                          not_applicable   \n",
       "...                                                   ...   \n",
       "109914                                     not_applicable   \n",
       "109915                                     not_applicable   \n",
       "109916  {'neg': 0.133, 'neu': 0.867, 'pos': 0.0, 'comp...   \n",
       "109917  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "109918  {'neg': 0.0, 'neu': 0.288, 'pos': 0.712, 'comp...   \n",
       "\n",
       "       vader_sentiment_sentence vader_sentiment_clause_A  \\\n",
       "0                      negative           not_applicable   \n",
       "1                      positive                 positive   \n",
       "2                      negative           not_applicable   \n",
       "3                      negative                  neutral   \n",
       "4                      positive           not_applicable   \n",
       "...                         ...                      ...   \n",
       "109914                 negative           not_applicable   \n",
       "109915                 positive           not_applicable   \n",
       "109916                 negative                 positive   \n",
       "109917                 positive                 positive   \n",
       "109918                 positive                 positive   \n",
       "\n",
       "       vader_sentiment_clause_B rule_structure      rule_label        contrast  \n",
       "0                not_applicable   no_structure  not_applicable  not_applicable  \n",
       "1                      positive        A-but-B         A-but-B     no_contrast  \n",
       "2                not_applicable   no_structure  not_applicable  not_applicable  \n",
       "3                      negative        A-yet-B         A-yet-B        contrast  \n",
       "4                not_applicable   no_structure  not_applicable  not_applicable  \n",
       "...                         ...            ...             ...             ...  \n",
       "109914           not_applicable   no_structure  not_applicable  not_applicable  \n",
       "109915           not_applicable   no_structure  not_applicable  not_applicable  \n",
       "109916                 negative        A-but-B         A-but-B        contrast  \n",
       "109917                  neutral      A-while-B       A-while-B        contrast  \n",
       "109918                 positive        A-yet-B         A-yet-B     no_contrast  \n",
       "\n",
       "[109919 rows x 15 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('datasets/covid19-twitter.tsv', sep=\"\\t\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a54603-7675-4173-a2e2-e61fcdba6a15",
   "metadata": {},
   "source": [
    "### Hydrate tweets from tweet-IDs using tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15118027-8464-4128-835e-ae23ca5a7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "bearer_token=\"YOUR_BEARER_TOKEN_VALUE\"\n",
    "api_key = 'YOUR_API_KEY_VALUE'\n",
    "api_secret = 'YOUR_API_SECRET_VALUE'\n",
    "access_token = 'YOUR_ACCESS_TOKEN_VALUE'\n",
    "access_token_secret = 'YOUR_ACCESS_TOKEN_SECRET_VALUE'\n",
    "\n",
    "client = tweepy.Client(bearer_token)\n",
    "client = tweepy.Client(consumer_key=consumer_key, \n",
    "                       consumer_secret=consumer_secret,\n",
    "                       access_token=access_token, \n",
    "                       access_token_secret=access_token_secret)\n",
    "\n",
    "tweet_ids = list(dataset[\"tweet_id\"])\n",
    "response = client.get_tweets(tweet_ids)\n",
    "\n",
    "tweets = []\n",
    "for index, tweet in enumerate(response.data):\n",
    "    assert(tweet.id==tweet_ids[index])\n",
    "    tweets.append(tweet.text)\n",
    "dataset[\"tweet\"] = tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37ba3184-6d70-4c3d-a8d4-0497b065f8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/fc/a5/4d82be566f069d7a9a702dcdf6f9106df0e0b042e738043c0cc7ddd7e3f6/pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/9c/3d/a121f284241f08268b21359bd425f7d4825cffc5ac5cd0e1b3d82ffd2b10/pytz-2024.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Obtaining dependency information for tzdata>=2022.7 from https://files.pythonhosted.org/packages/65/58/f9c9e6be752e9fcb8b6a0ee9fb87e6e7a1f6bcab2cdc73f02bb7ba91ada0/tzdata-2024.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.2 pytz-2024.1 tzdata-2024.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Collecting tqdm\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/18/eb/fdb7eb9e48b7b02554e1664afd3bd3f117f6b6d6c5881438a0b055554f9b/tqdm-4.66.4-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m827.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m101.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Collecting tweet-preprocessor\n",
      "  Obtaining dependency information for tweet-preprocessor from https://files.pythonhosted.org/packages/17/9d/71bd016a9edcef8860c607e531f30bd09b13103c7951ae73dd2bf174163c/tweet_preprocessor-0.6.0-py3-none-any.whl.metadata\n",
      "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: tweet-preprocessor\n",
      "Successfully installed tweet-preprocessor-0.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/46/c0/63d3a8da39a2ee051df229111aa93f6dca2b56f8080abd34993938166455/scikit_learn-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading scikit_learn-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.0)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Obtaining dependency information for scipy>=1.6.0 from https://files.pythonhosted.org/packages/36/07/035d22ff9795129c5a847c64cb43c1fa9188826b59344fee28a3ab02e283/scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m197.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Obtaining dependency information for joblib>=1.2.0 from https://files.pythonhosted.org/packages/91/29/df4b9b42f2be0b623cbd5e2140cafcaa2bef0759a00b7b70104dcfe2fb51/joblib-1.4.2-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=3.1.0 from https://files.pythonhosted.org/packages/4b/2c/ffbf7a134b9ab11a67b0cf0726453cedd9c5043a4fe7a35d1cefa9a1bcfb/threadpoolctl-3.5.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.0 scipy-1.13.1 threadpoolctl-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Collecting lime\n",
      "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m302.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from lime) (3.8.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from lime) (1.26.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lime) (1.13.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from lime) (4.66.4)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.11/dist-packages (from lime) (1.5.0)\n",
      "Collecting scikit-image>=0.12 (from lime)\n",
      "  Obtaining dependency information for scikit-image>=0.12 from https://files.pythonhosted.org/packages/0a/40/2c57864acd77c168b96cb6e4e62651b9c98733962793293991ef55e2982c/scikit_image-0.23.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading scikit_image-0.23.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting networkx>=2.8 (from scikit-image>=0.12->lime)\n",
      "  Obtaining dependency information for networkx>=2.8 from https://files.pythonhosted.org/packages/38/e9/5f72929373e1a0e8d142a130f3f97e6ff920070f87f91c4e13e40e0fba5a/networkx-3.3-py3-none-any.whl.metadata\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (10.0.1)\n",
      "Collecting imageio>=2.33 (from scikit-image>=0.12->lime)\n",
      "  Obtaining dependency information for imageio>=2.33 from https://files.pythonhosted.org/packages/a3/b6/39c7dad203d9984225f47e0aa39ac3ba3a47c77a02d0ef2a7be691855a06/imageio-2.34.1-py3-none-any.whl.metadata\n",
      "  Downloading imageio-2.34.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image>=0.12->lime)\n",
      "  Obtaining dependency information for tifffile>=2022.8.12 from https://files.pythonhosted.org/packages/d9/6c/740c07588434e86028c24b0653c1eb6b46904d9ce585a20f07590620ec41/tifffile-2024.5.22-py3-none-any.whl.metadata\n",
      "  Downloading tifffile-2024.5.22-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (23.1)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image>=0.12->lime)\n",
      "  Obtaining dependency information for lazy-loader>=0.4 from https://files.pythonhosted.org/packages/83/60/d497a310bde3f01cb805196ac61b7ad6dc5dcf8dce66634dc34364b20b4f/lazy_loader-0.4-py3-none-any.whl.metadata\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->lime) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
      "Downloading scikit_image-0.23.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading imageio-2.34.1-py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.5/313.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tifffile-2024.5.22-py3-none-any.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.5/225.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: lime\n",
      "  Building wheel for lime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=8f5af970516cc7886395c550cf9c1a4eea0dc60c9944dd133b8a39626682646e\n",
      "  Stored in directory: /root/.cache/pip/wheels/85/fa/a3/9c2d44c9f3cd77cf4e533b58900b2bf4487f2a17e8ec212a3d\n",
      "Successfully built lime\n",
      "Installing collected packages: tifffile, networkx, lazy-loader, imageio, scikit-image, lime\n",
      "Successfully installed imageio-2.34.1 lazy-loader-0.4 lime-0.2.0.1 networkx-3.3 scikit-image-0.23.2 tifffile-2024.5.22\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Collecting keras-tuner\n",
      "  Obtaining dependency information for keras-tuner from https://files.pythonhosted.org/packages/db/5d/945296512980b0827e93418514c8be9236baa6f0a1e8ca8be3a2026665b0/keras_tuner-1.4.7-py3-none-any.whl.metadata\n",
      "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.14.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (23.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.31.0)\n",
      "Collecting kt-legacy (from keras-tuner)\n",
      "  Obtaining dependency information for kt-legacy from https://files.pythonhosted.org/packages/16/53/aca9f36da2516db008017db85a1f3cafaee0efc5fc7a25d94c909651792f/kt_legacy-1.0.5-py3-none-any.whl.metadata\n",
      "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2023.7.22)\n",
      "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
      "Installing collected packages: kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/d9/b7/98f821d70102e2d38483bbb7013a689d2d646daa4495377bc910374ad727/transformers-4.41.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m367.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from transformers)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/41/24/0b023b6537dfc9bae2c779353998e3e99ac7dfff4222fc6126650e93c3f3/filelock-3.14.0-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.23.0 from https://files.pythonhosted.org/packages/66/e8/bbbad5c7b49c68def42830f96c606e693bfa935a886740a363f04cb84e44/huggingface_hub-0.23.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/39/29/8158a6e69e97b9c72fab0b46fe4d57c789d07ef91fe4afde23721e7cac61/regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m574.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.20,>=0.19 from https://files.pythonhosted.org/packages/a7/03/fb50fc03f86016b227a967c8d474f90230c885c0d18f78acdfda7a96ce56/tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.4.1 from https://files.pythonhosted.org/packages/d5/85/1e7d2804cbf82204cde462d16f1cb0ff5814b03f559fb46ceaa6b7020db4/safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.66.4)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.0->transformers)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/8f/df/de2c06b316142063b6ccccc97cdc54185e3af771aa4f056d56f0db0e3466/fsspec-2024.6.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2024.6.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.0/785.0 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.14.0-py3-none-any.whl (12 kB)\n",
      "Downloading fsspec-2024.6.0-py3-none-any.whl (176 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.9/176.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.14.0 fsspec-2024.6.0 huggingface-hub-0.23.3 regex-2024.5.15 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.41.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Install packages\n",
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "!pip install tweet-preprocessor\n",
    "!pip install scikit-learn\n",
    "!pip install lime\n",
    "!pip install keras-tuner\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3855632-a4c3-404f-8fac-f54aa0fa12f5",
   "metadata": {},
   "source": [
    "### Reproduce benchmark results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c71ea5ec-6e34-4a0f-bcf3-84c95926ab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifiy the parameters\n",
    "config = {\n",
    "            \"asset_name\":\"bertweet_lstm_reproducibility_test\",\n",
    "            \"model_name\":\"bertweet_lstm\",\n",
    "            \"seed_value\":11,\n",
    "            \"dataset_name\":\"covid19-twitter\",\n",
    "            \"train_model\":True,\n",
    "            \"evaluate_model\":True,\n",
    "            \"generate_explanations\":True,\n",
    "            \"generate_explanation_for_one_instance\":True,\n",
    "            \"fine_tune_word_embeddings\":True, # for cwe classifiers: True, for flat classifiers: False\n",
    "            \"optimizer\":\"adam\",\n",
    "            \"learning_rate\":1e-6,\n",
    "            \"mini_batch_size\":50,\n",
    "            \"train_epochs\":2,\n",
    "            \"dropout\":0.5,\n",
    "            \"lime_no_of_samples\":1000,\n",
    "            \"hidden_units\":128  #for mlp, lstm, bilstm, gru, bigru: 128, for cnn: 100 (no of filters), for transformer: 2 (no of heads)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e9df2fb-4754-4cfa-9ff8-8a5526f47660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 13:56:02.039850: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-11 13:56:02.039889: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-11 13:56:02.039915: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-11 13:56:02.047803: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Num GPUs Available:  1\n",
      "\n",
      "Creating input data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 13:57:03.266389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9689 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word vectors created\n",
      "\n",
      "Converted 36063 words (27654 misses)\n",
      "\n",
      "Building model\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 81)]                 0         []                            \n",
      "                                                                                                  \n",
      " tf.cast (TFOpLambda)        (None, 81)                   0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " tf.math.not_equal (TFOpLam  (None, 81)                   0         ['tf.cast[0][0]']             \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.cast_1 (TFOpLambda)      (None, 81)                   0         ['tf.math.not_equal[0][0]']   \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobert  TFBaseModelOutputWithPooli   1348999   ['tf.cast[0][0]',             \n",
      " aModel)                     ngAndCrossAttentions(last_   68         'tf.cast_1[0][0]']           \n",
      "                             hidden_state=(None, 81, 76                                           \n",
      "                             8),                                                                  \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " classifier (LSTM)           (None, 128)                  459264    ['tf_roberta_model[0][0]']    \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1)                    129       ['classifier[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 135359361 (516.35 MB)\n",
      "Trainable params: 135359361 (516.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 13:58:39.929288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 725s 504ms/step - loss: 0.5685 - accuracy: 0.7064 - val_loss: 0.4643 - val_accuracy: 0.7947\n",
      "Epoch 2/2\n",
      "1407/1407 [==============================] - 711s 505ms/step - loss: 0.4009 - accuracy: 0.8234 - val_loss: 0.3549 - val_accuracy: 0.8522\n",
      "687/687 [==============================] - 65s 95ms/step - loss: 0.3632 - accuracy: 0.8482\n",
      "test loss, test acc: [0.36320289969444275, 0.8482077717781067]\n",
      "687/687 [==============================] - 45s 63ms/step\n",
      "21984\n",
      "\n",
      "\n",
      "0.876\n",
      "\n",
      "LIME explanations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                               | 0/10677 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                               | 0/10677 [00:07<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.004445814143050769, 0.0028624276012147003, 0.0016117015342697592, 0.017219550437597812, 0.04531795744923671, 0.00487914033626523, 0.0010832163857604387, 0.007237661962911473, 0.0037952153545474454, 0.03624552448186694, 8.56537211084601e-05, 0.0028624276012147003, 0.0016542142841297985, 0.0025711487789064737, 0.006174705142321824, 0.005545352106510327, 0.022213339709194774, 0.016792857637981072, 0.00893785926095322, 0.0726572688371982, 0.017913379712691378, 0.01704895982995209, 0.012045332214685579, 0.006174705142321824, 0.0020007412120077502, 0.00251121497502714, 0.013192628479645403, 0.051964313550828194, 0.006174705142321824, 0.02055892118744381, 0.0023240357383178, 0.008235041358247281, 0.007622891505037446, 0.0037952153545474454, 0.00481337003115552, 0.00031192927079150523, 0.05479203322186133, 0.025610590919955677, 0.0037952153545474454, 0.05479203322186133, 0.0028624276012147003, 0.009670897657175761, 0.0005617663833460786, 0.0007782037635891297, 0.05479203322186133, 0.0726572688371982, 0.02055892118744381, 0.0008894951677876764, 0.13248163344449962, 0.001761890786261321]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import argparse\n",
    "import pprint\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "print(\"\\nNum GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "import argparse\n",
    "import subprocess as sp\n",
    "import distutils\n",
    "import pprint\n",
    "\n",
    "#change the code execution directory to current directory\n",
    "os.chdir(os.getcwd())\n",
    "\n",
    "#scripts\n",
    "from scripts.preprocess_dataset import Preprocess_dataset\n",
    "from scripts.word_vectors import Word_vectors\n",
    "from scripts.dataset_division import Dataset_division\n",
    "\n",
    "from scripts.train_mlp import train_mlp\n",
    "from scripts.train_transformer import train_transformer\n",
    "from scripts.train_cnn import train_cnn\n",
    "from scripts.train_rnn import train_rnn\n",
    "\n",
    "from scripts.train_bertweet_mlp import train_bertweet_mlp\n",
    "from scripts.train_bertweet_transformer import train_bertweet_transformer\n",
    "from scripts.train_bertweet_cnn import train_bertweet_cnn\n",
    "from scripts.train_bertweet_rnn import train_bertweet_rnn\n",
    "# from scripts.train_gpt2_mlp import train_gpt2_mlp\n",
    "# from scripts.train_gpt2_transformer import train_gpt2_transformer\n",
    "# from scripts.train_gpt2_rnn import train_gpt2_rnn\n",
    "# from scripts.train_gpt2_cnn import train_gpt2_cnn\n",
    "# from scripts.train_roberta_mlp import train_roberta_mlp\n",
    "# from scripts.train_roberta_cnn import train_roberta_cnn\n",
    "# from scripts.train_roberta_transformer import train_roberta_transformer\n",
    "# from scripts.train_roberta_rnn import train_roberta_rnn\n",
    "# from scripts.train_xlnet_mlp import train_xlnet_mlp\n",
    "# from scripts.train_xlnet_cnn import train_xlnet_cnn\n",
    "# from scripts.train_xlnet_transformer import train_xlnet_transformer\n",
    "# from scripts.train_xlnet_rnn import train_xlnet_rnn\n",
    "# from scripts.train_distilbert_mlp import train_distilbert_mlp\n",
    "# from scripts.train_distilbert_cnn import train_distilbert_cnn\n",
    "# from scripts.train_distilbert_transformer import train_distilbert_transformer\n",
    "# from scripts.train_distilbert_rnn import train_distilbert_rnn\n",
    "# from scripts.train_elmo_mlp import train_elmo_mlp\n",
    "\n",
    "#disable warnings\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#set the gpu execution device with highest free memory\n",
    "def mask_unused_gpus(leave_unmasked=1): # No of avaialbe GPUs on the system\n",
    "    COMMAND = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    try:\n",
    "        _output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
    "        memory_free_info = _output_to_list(sp.check_output(COMMAND.split()))[1:]\n",
    "        memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "        available_gpus = [i for i, x in enumerate(memory_free_values)]\n",
    "        if len(available_gpus) < leave_unmasked: raise ValueError('Found only %d usable GPUs in the system' % len(available_gpus))\n",
    "        gpu_with_highest_free_memory = 0\n",
    "        highest_free_memory = 0\n",
    "        for index, memory in enumerate(memory_free_values):\n",
    "            if memory > highest_free_memory:\n",
    "                highest_free_memory = memory\n",
    "                gpu_with_highest_free_memory = index\n",
    "        return str(gpu_with_highest_free_memory)\n",
    "    except Exception as e:\n",
    "        print('\"nvidia-smi\" is probably not installed. GPUs are not masked', e)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = mask_unused_gpus()\n",
    "\n",
    "#prepare the dataset\n",
    "print(\"\\nCreating input data\")\n",
    "preprocessed_dataset = Preprocess_dataset(config).preprocess_covid_tweets(dataset)\n",
    "word_vectors, word_index = Word_vectors(config).create_word_vectors(preprocessed_dataset)\n",
    "train_dataset, val_datasets, test_datasets = Dataset_division(config).train_val_test_split(preprocessed_dataset)\n",
    "\n",
    "#create model\n",
    "print(\"\\nBuilding model\")\n",
    "\n",
    "#flat classifiers\n",
    "if config[\"model_name\"] == \"mlp\":\n",
    "    train_mlp(config).train_model(train_dataset, val_datasets, test_datasets, word_index, word_vectors)\n",
    "elif config[\"model_name\"] == \"transformer\":\n",
    "    train_transformer(config).train_model(train_dataset, val_datasets, test_datasets, word_index, word_vectors)\n",
    "elif config[\"model_name\"] == \"cnn\":\n",
    "    train_cnn(config).train_model(train_dataset, val_datasets, test_datasets, word_index, word_vectors)\n",
    "elif config[\"model_name\"] in [\"lstm\", \"bilstm\", \"gru\", \"bigru\"]:\n",
    "    train_rnn(config).train_model(train_dataset, val_datasets, test_datasets, word_index, word_vectors)\n",
    "\n",
    "#cwe classifiers\n",
    "elif config[\"model_name\"] == \"bertweet_mlp\":\n",
    "    train_bertweet_mlp(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"bertweet_transformer\":\n",
    "    train_bertweet_transformer(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"bertweet_cnn\":\n",
    "    train_bertweet_cnn(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] in [\"bertweet_lstm\", \"bertweet_bilstm\", \"bertweet_gru\", \"bertweet_bigru\"]:\n",
    "    train_bertweet_rnn(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"gpt2_mlp\":\n",
    "    train_gpt2_mlp(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"gpt2_transformer\":\n",
    "    train_gpt2_transformer(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] in [\"gpt2_lstm\", \"gpt2_bilstm\", \"gpt2_gru\", \"gpt2_bigru\"]:\n",
    "    train_gpt2_rnn(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"gpt2_cnn\":\n",
    "    train_gpt2_cnn(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"roberta_mlp\":\n",
    "    train_roberta_mlp(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"roberta_cnn\":\n",
    "    train_roberta_cnn(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"roberta_transformer\":\n",
    "    train_roberta_transformer(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] in [\"roberta_lstm\", \"roberta_bilstm\", \"roberta_gru\", \"roberta_bigru\"]:\n",
    "    train_roberta_rnn(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"xlnet_mlp\":\n",
    "    train_xlnet_mlp(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"xlnet_cnn\":\n",
    "    train_xlnet_cnn(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"xlnet_transformer\":\n",
    "    train_xlnet_transformer(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] in [\"xlnet_lstm\", \"xlnet_bilstm\", \"xlnet_gru\", \"xlnet_bigru\"]:\n",
    "    train_xlnet_rnn(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"distilbert_mlp\":\n",
    "    train_distilbert_mlp(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"distilbert_cnn\":\n",
    "    train_distilbert_cnn(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"distilbert_transformer\":\n",
    "    train_distilbert_transformer(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] in [\"distilbert_lstm\", \"distilbert_bilstm\", \"distilbert_gru\", \"distilbert_bigru\"]:\n",
    "    train_distilbert_rnn(config).train_model(train_dataset, val_datasets, test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04cf324-139d-4e59-983e-3ba234bb425f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
