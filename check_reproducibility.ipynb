{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be2c8b4a-55ee-4e04-8d11-3462a6699f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.0rc1\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8a8c0d3-2372-48f1-95ae-fe721f135a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No LSB modules are available.\n",
      "Distributor ID:\tUbuntu\n",
      "Description:\tUbuntu 22.04.2 LTS\n",
      "Release:\t22.04\n",
      "Codename:\tjammy\n"
     ]
    }
   ],
   "source": [
    "!lsb_release -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "817729af-40f0-4f71-a227-6318f0100eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 12 07:26:09 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2080 Ti     On  | 00000000:65:00.0 Off |                  N/A |\n",
      "|  0%   34C    P8              21W / 300W |   9989MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c20d50-d253-4250-b35f-061333227413",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9cc3c3-2855-496f-b7d4-a868773fce6d",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb45c2d8-b323-4790-8ce1-1dcaa6d82b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emojis</th>\n",
       "      <th>emoji_names</th>\n",
       "      <th>emotion_scores</th>\n",
       "      <th>agg_emotion_score</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>vader_score_sentence</th>\n",
       "      <th>vader_score_clause_A</th>\n",
       "      <th>vader_score_clause_B</th>\n",
       "      <th>vader_sentiment_sentence</th>\n",
       "      <th>vader_sentiment_clause_A</th>\n",
       "      <th>vader_sentiment_clause_B</th>\n",
       "      <th>rule_structure</th>\n",
       "      <th>rule_label</th>\n",
       "      <th>contrast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1292641373758328839</td>\n",
       "      <td>['😢', '😢']</td>\n",
       "      <td>['crying face', 'crying face']</td>\n",
       "      <td>[[-0.25, -0.36, -0.5, 0.0, -1.0, 0.11], [-0.25...</td>\n",
       "      <td>-4.00</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.249, 'neu': 0.751, 'pos': 0.0, 'comp...</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>negative</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>no_structure</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1377497788460720129</td>\n",
       "      <td>['😂', '😂', '😂', '😂']</td>\n",
       "      <td>['face with tears of joy', 'face with tears of...</td>\n",
       "      <td>[[0.0, -0.06, -0.06, 0.94, 0.0, 0.22], [0.0, -...</td>\n",
       "      <td>4.16</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.021, 'neu': 0.909, 'pos': 0.07, 'com...</td>\n",
       "      <td>{'neg': 0.036, 'neu': 0.909, 'pos': 0.056, 'co...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.9, 'pos': 0.1, 'compound...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>A-but-B</td>\n",
       "      <td>A-but-B</td>\n",
       "      <td>no_contrast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1318434591322087427</td>\n",
       "      <td>['💔', '💔', '💔']</td>\n",
       "      <td>['broken heart', 'broken heart', 'broken heart']</td>\n",
       "      <td>[[-0.39, -0.33, -0.14, 0.0, -0.94, 0.17], [-0....</td>\n",
       "      <td>-4.89</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.176, 'neu': 0.824, 'pos': 0.0, 'comp...</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>negative</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>no_structure</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1303627865796222976</td>\n",
       "      <td>['😠', '😠', '😠', '😠', '😠']</td>\n",
       "      <td>['angry face', 'angry face', 'angry face', 'an...</td>\n",
       "      <td>[[-1.0, -0.56, -0.17, 0.0, -0.25, 0.08], [-1.0...</td>\n",
       "      <td>-9.50</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.117, 'neu': 0.883, 'pos': 0.0, 'comp...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>{'neg': 0.302, 'neu': 0.698, 'pos': 0.0, 'comp...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>A-yet-B</td>\n",
       "      <td>A-yet-B</td>\n",
       "      <td>contrast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1361789228426932227</td>\n",
       "      <td>['🎉', '🎉', '🎉']</td>\n",
       "      <td>['party popper', 'party popper', 'party popper']</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.92, 0.0, 0.33], [0.0, 0.0, ...</td>\n",
       "      <td>3.75</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.686, 'pos': 0.314, 'comp...</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>no_structure</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109914</th>\n",
       "      <td>1304993476564590592</td>\n",
       "      <td>['😡', '😡', '😡']</td>\n",
       "      <td>['pouting face', 'pouting face', 'pouting face']</td>\n",
       "      <td>[[-1.0, -0.56, -0.11, 0.0, -0.36, 0.06], [-1.0...</td>\n",
       "      <td>-5.91</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.192, 'neu': 0.808, 'pos': 0.0, 'comp...</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>negative</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>no_structure</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109915</th>\n",
       "      <td>1389249469850394627</td>\n",
       "      <td>['💐', '💐', '💐', '💐', '👍']</td>\n",
       "      <td>['bouquet', 'bouquet', 'bouquet', 'bouquet', '...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.69, -0.11, 0.58], [0.0, 0.0...</td>\n",
       "      <td>5.19</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.291, 'pos': 0.709, 'comp...</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>positive</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>no_structure</td>\n",
       "      <td>not_applicable</td>\n",
       "      <td>not_applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109916</th>\n",
       "      <td>1322678255128903683</td>\n",
       "      <td>['😢', '😢']</td>\n",
       "      <td>['crying face', 'crying face']</td>\n",
       "      <td>[[-0.25, -0.36, -0.5, 0.0, -1.0, 0.11], [-0.25...</td>\n",
       "      <td>-4.00</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.106, 'neu': 0.846, 'pos': 0.049, 'co...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.822, 'pos': 0.178, 'comp...</td>\n",
       "      <td>{'neg': 0.133, 'neu': 0.867, 'pos': 0.0, 'comp...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>A-but-B</td>\n",
       "      <td>A-but-B</td>\n",
       "      <td>contrast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109917</th>\n",
       "      <td>1361083777724915712</td>\n",
       "      <td>['🙏', '🙏', '🙏', '🙏', '🙏', '🙏', '🙏', '🙏', '🙏', ...</td>\n",
       "      <td>['folded hands', 'folded hands', 'folded hands...</td>\n",
       "      <td>[[-0.06, 0.0, -0.11, 0.25, -0.11, 0.58], [-0.0...</td>\n",
       "      <td>5.50</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.79, 'pos': 0.21, 'compou...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.748, 'pos': 0.252, 'comp...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>A-while-B</td>\n",
       "      <td>A-while-B</td>\n",
       "      <td>contrast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109918</th>\n",
       "      <td>1286590732728377344</td>\n",
       "      <td>['🌺', '💖', '✨', '💫']</td>\n",
       "      <td>['hibiscus', 'sparkling heart', 'sparkles', 'd...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.39, 0.0, 0.19], [0.0, 0.0, ...</td>\n",
       "      <td>3.27</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.752, 'pos': 0.248, 'comp...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.88, 'pos': 0.12, 'compou...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.288, 'pos': 0.712, 'comp...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>A-yet-B</td>\n",
       "      <td>A-yet-B</td>\n",
       "      <td>no_contrast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109919 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   tweet_id  \\\n",
       "0       1292641373758328839   \n",
       "1       1377497788460720129   \n",
       "2       1318434591322087427   \n",
       "3       1303627865796222976   \n",
       "4       1361789228426932227   \n",
       "...                     ...   \n",
       "109914  1304993476564590592   \n",
       "109915  1389249469850394627   \n",
       "109916  1322678255128903683   \n",
       "109917  1361083777724915712   \n",
       "109918  1286590732728377344   \n",
       "\n",
       "                                                   emojis  \\\n",
       "0                                              ['😢', '😢']   \n",
       "1                                    ['😂', '😂', '😂', '😂']   \n",
       "2                                         ['💔', '💔', '💔']   \n",
       "3                               ['😠', '😠', '😠', '😠', '😠']   \n",
       "4                                         ['🎉', '🎉', '🎉']   \n",
       "...                                                   ...   \n",
       "109914                                    ['😡', '😡', '😡']   \n",
       "109915                          ['💐', '💐', '💐', '💐', '👍']   \n",
       "109916                                         ['😢', '😢']   \n",
       "109917  ['🙏', '🙏', '🙏', '🙏', '🙏', '🙏', '🙏', '🙏', '🙏', ...   \n",
       "109918                               ['🌺', '💖', '✨', '💫']   \n",
       "\n",
       "                                              emoji_names  \\\n",
       "0                          ['crying face', 'crying face']   \n",
       "1       ['face with tears of joy', 'face with tears of...   \n",
       "2        ['broken heart', 'broken heart', 'broken heart']   \n",
       "3       ['angry face', 'angry face', 'angry face', 'an...   \n",
       "4        ['party popper', 'party popper', 'party popper']   \n",
       "...                                                   ...   \n",
       "109914   ['pouting face', 'pouting face', 'pouting face']   \n",
       "109915  ['bouquet', 'bouquet', 'bouquet', 'bouquet', '...   \n",
       "109916                     ['crying face', 'crying face']   \n",
       "109917  ['folded hands', 'folded hands', 'folded hands...   \n",
       "109918  ['hibiscus', 'sparkling heart', 'sparkles', 'd...   \n",
       "\n",
       "                                           emotion_scores  agg_emotion_score  \\\n",
       "0       [[-0.25, -0.36, -0.5, 0.0, -1.0, 0.11], [-0.25...              -4.00   \n",
       "1       [[0.0, -0.06, -0.06, 0.94, 0.0, 0.22], [0.0, -...               4.16   \n",
       "2       [[-0.39, -0.33, -0.14, 0.0, -0.94, 0.17], [-0....              -4.89   \n",
       "3       [[-1.0, -0.56, -0.17, 0.0, -0.25, 0.08], [-1.0...              -9.50   \n",
       "4       [[0.0, 0.0, 0.0, 0.92, 0.0, 0.33], [0.0, 0.0, ...               3.75   \n",
       "...                                                   ...                ...   \n",
       "109914  [[-1.0, -0.56, -0.11, 0.0, -0.36, 0.06], [-1.0...              -5.91   \n",
       "109915  [[0.0, 0.0, 0.0, 0.69, -0.11, 0.58], [0.0, 0.0...               5.19   \n",
       "109916  [[-0.25, -0.36, -0.5, 0.0, -1.0, 0.11], [-0.25...              -4.00   \n",
       "109917  [[-0.06, 0.0, -0.11, 0.25, -0.11, 0.58], [-0.0...               5.50   \n",
       "109918  [[0.0, 0.0, 0.0, 0.39, 0.0, 0.19], [0.0, 0.0, ...               3.27   \n",
       "\n",
       "       sentiment_label                               vader_score_sentence  \\\n",
       "0             negative  {'neg': 0.249, 'neu': 0.751, 'pos': 0.0, 'comp...   \n",
       "1             positive  {'neg': 0.021, 'neu': 0.909, 'pos': 0.07, 'com...   \n",
       "2             negative  {'neg': 0.176, 'neu': 0.824, 'pos': 0.0, 'comp...   \n",
       "3             negative  {'neg': 0.117, 'neu': 0.883, 'pos': 0.0, 'comp...   \n",
       "4             positive  {'neg': 0.0, 'neu': 0.686, 'pos': 0.314, 'comp...   \n",
       "...                ...                                                ...   \n",
       "109914        negative  {'neg': 0.192, 'neu': 0.808, 'pos': 0.0, 'comp...   \n",
       "109915        positive  {'neg': 0.0, 'neu': 0.291, 'pos': 0.709, 'comp...   \n",
       "109916        negative  {'neg': 0.106, 'neu': 0.846, 'pos': 0.049, 'co...   \n",
       "109917        positive  {'neg': 0.0, 'neu': 0.79, 'pos': 0.21, 'compou...   \n",
       "109918        positive  {'neg': 0.0, 'neu': 0.752, 'pos': 0.248, 'comp...   \n",
       "\n",
       "                                     vader_score_clause_A  \\\n",
       "0                                          not_applicable   \n",
       "1       {'neg': 0.036, 'neu': 0.909, 'pos': 0.056, 'co...   \n",
       "2                                          not_applicable   \n",
       "3       {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "4                                          not_applicable   \n",
       "...                                                   ...   \n",
       "109914                                     not_applicable   \n",
       "109915                                     not_applicable   \n",
       "109916  {'neg': 0.0, 'neu': 0.822, 'pos': 0.178, 'comp...   \n",
       "109917  {'neg': 0.0, 'neu': 0.748, 'pos': 0.252, 'comp...   \n",
       "109918  {'neg': 0.0, 'neu': 0.88, 'pos': 0.12, 'compou...   \n",
       "\n",
       "                                     vader_score_clause_B  \\\n",
       "0                                          not_applicable   \n",
       "1       {'neg': 0.0, 'neu': 0.9, 'pos': 0.1, 'compound...   \n",
       "2                                          not_applicable   \n",
       "3       {'neg': 0.302, 'neu': 0.698, 'pos': 0.0, 'comp...   \n",
       "4                                          not_applicable   \n",
       "...                                                   ...   \n",
       "109914                                     not_applicable   \n",
       "109915                                     not_applicable   \n",
       "109916  {'neg': 0.133, 'neu': 0.867, 'pos': 0.0, 'comp...   \n",
       "109917  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "109918  {'neg': 0.0, 'neu': 0.288, 'pos': 0.712, 'comp...   \n",
       "\n",
       "       vader_sentiment_sentence vader_sentiment_clause_A  \\\n",
       "0                      negative           not_applicable   \n",
       "1                      positive                 positive   \n",
       "2                      negative           not_applicable   \n",
       "3                      negative                  neutral   \n",
       "4                      positive           not_applicable   \n",
       "...                         ...                      ...   \n",
       "109914                 negative           not_applicable   \n",
       "109915                 positive           not_applicable   \n",
       "109916                 negative                 positive   \n",
       "109917                 positive                 positive   \n",
       "109918                 positive                 positive   \n",
       "\n",
       "       vader_sentiment_clause_B rule_structure      rule_label        contrast  \n",
       "0                not_applicable   no_structure  not_applicable  not_applicable  \n",
       "1                      positive        A-but-B         A-but-B     no_contrast  \n",
       "2                not_applicable   no_structure  not_applicable  not_applicable  \n",
       "3                      negative        A-yet-B         A-yet-B        contrast  \n",
       "4                not_applicable   no_structure  not_applicable  not_applicable  \n",
       "...                         ...            ...             ...             ...  \n",
       "109914           not_applicable   no_structure  not_applicable  not_applicable  \n",
       "109915           not_applicable   no_structure  not_applicable  not_applicable  \n",
       "109916                 negative        A-but-B         A-but-B        contrast  \n",
       "109917                  neutral      A-while-B       A-while-B        contrast  \n",
       "109918                 positive        A-yet-B         A-yet-B     no_contrast  \n",
       "\n",
       "[109919 rows x 15 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('datasets/covid19-twitter.tsv', sep=\"\\t\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a54603-7675-4173-a2e2-e61fcdba6a15",
   "metadata": {},
   "source": [
    "### Hydrate tweets from tweet-IDs using tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15118027-8464-4128-835e-ae23ca5a7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "bearer_token=\"YOUR_BEARER_TOKEN_VALUE\"\n",
    "api_key = 'YOUR_API_KEY_VALUE'\n",
    "api_secret = 'YOUR_API_SECRET_VALUE'\n",
    "access_token = 'YOUR_ACCESS_TOKEN_VALUE'\n",
    "access_token_secret = 'YOUR_ACCESS_TOKEN_SECRET_VALUE'\n",
    "\n",
    "client = tweepy.Client(bearer_token)\n",
    "client = tweepy.Client(consumer_key=consumer_key, \n",
    "                       consumer_secret=consumer_secret,\n",
    "                       access_token=access_token, \n",
    "                       access_token_secret=access_token_secret)\n",
    "\n",
    "tweet_ids = list(dataset[\"tweet_id\"])\n",
    "response = client.get_tweets(tweet_ids)\n",
    "\n",
    "tweets = []\n",
    "for index, tweet in enumerate(response.data):\n",
    "    assert(tweet.id==tweet_ids[index])\n",
    "    tweets.append(tweet.text)\n",
    "dataset[\"tweet\"] = tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3855632-a4c3-404f-8fac-f54aa0fa12f5",
   "metadata": {},
   "source": [
    "### Option 1: Reproduce benchmark results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353914ff-d298-4d27-843b-023d276e56c7",
   "metadata": {},
   "source": [
    "#### Specifiy the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c71ea5ec-6e34-4a0f-bcf3-84c95926ab2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'asset_name': 'mlp_1e-6LearningRate_200Epochs_0.5Dropout',\n",
      " 'dataset_name': 'covid19-twitter',\n",
      " 'dataset_path': 'datasets/covid19-twitter/raw_dataset.pickle',\n",
      " 'dropout': 0.5,\n",
      " 'evaluate_model': True,\n",
      " 'fine_tune_word_embeddings': False,\n",
      " 'generate_explanation': True,\n",
      " 'generate_explanation_for_one_instance': False,\n",
      " 'hidden_units': 128,\n",
      " 'learning_rate': 1e-06,\n",
      " 'lime_no_of_samples': 1000,\n",
      " 'mini_batch_size': 50,\n",
      " 'model_name': 'mlp',\n",
      " 'optimizer': 'adam',\n",
      " 'seed_value': 11,\n",
      " 'train_epochs': 200,\n",
      " 'train_model': True}\n",
      "\n",
      "\n",
      "{'asset_name': 'cnn_1e-6LearningRate_200Epochs_0.5Dropout',\n",
      " 'dataset_name': 'covid19-twitter',\n",
      " 'dataset_path': 'datasets/covid19-twitter/raw_dataset.pickle',\n",
      " 'dropout': 0.5,\n",
      " 'evaluate_model': True,\n",
      " 'fine_tune_word_embeddings': False,\n",
      " 'generate_explanation': True,\n",
      " 'generate_explanation_for_one_instance': False,\n",
      " 'hidden_units': 100,\n",
      " 'learning_rate': 1e-06,\n",
      " 'lime_no_of_samples': 1000,\n",
      " 'mini_batch_size': 50,\n",
      " 'model_name': 'cnn',\n",
      " 'optimizer': 'adam',\n",
      " 'seed_value': 11,\n",
      " 'train_epochs': 200,\n",
      " 'train_model': True}\n",
      "\n",
      "\n",
      "{'asset_name': 'lstm_1e-6LearningRate_200Epochs_0.5Dropout',\n",
      " 'dataset_name': 'covid19-twitter',\n",
      " 'dataset_path': 'datasets/covid19-twitter/raw_dataset.pickle',\n",
      " 'dropout': 0.5,\n",
      " 'evaluate_model': True,\n",
      " 'fine_tune_word_embeddings': False,\n",
      " 'generate_explanation': True,\n",
      " 'generate_explanation_for_one_instance': False,\n",
      " 'hidden_units': 128,\n",
      " 'learning_rate': 1e-06,\n",
      " 'lime_no_of_samples': 1000,\n",
      " 'mini_batch_size': 50,\n",
      " 'model_name': 'lstm',\n",
      " 'optimizer': 'adam',\n",
      " 'seed_value': 11,\n",
      " 'train_epochs': 200,\n",
      " 'train_model': True}\n",
      "\n",
      "\n",
      "{'asset_name': 'bilstm_1e-6LearningRate_200Epochs_0.5Dropout',\n",
      " 'dataset_name': 'covid19-twitter',\n",
      " 'dataset_path': 'datasets/covid19-twitter/raw_dataset.pickle',\n",
      " 'dropout': 0.5,\n",
      " 'evaluate_model': True,\n",
      " 'fine_tune_word_embeddings': False,\n",
      " 'generate_explanation': True,\n",
      " 'generate_explanation_for_one_instance': False,\n",
      " 'hidden_units': 128,\n",
      " 'learning_rate': 1e-06,\n",
      " 'lime_no_of_samples': 1000,\n",
      " 'mini_batch_size': 50,\n",
      " 'model_name': 'bilstm',\n",
      " 'optimizer': 'adam',\n",
      " 'seed_value': 11,\n",
      " 'train_epochs': 200,\n",
      " 'train_model': True}\n",
      "\n",
      "\n",
      "{'asset_name': 'gru_1e-6LearningRate_200Epochs_0.5Dropout',\n",
      " 'dataset_name': 'covid19-twitter',\n",
      " 'dataset_path': 'datasets/covid19-twitter/raw_dataset.pickle',\n",
      " 'dropout': 0.5,\n",
      " 'evaluate_model': True,\n",
      " 'fine_tune_word_embeddings': False,\n",
      " 'generate_explanation': True,\n",
      " 'generate_explanation_for_one_instance': False,\n",
      " 'hidden_units': 128,\n",
      " 'learning_rate': 1e-06,\n",
      " 'lime_no_of_samples': 1000,\n",
      " 'mini_batch_size': 50,\n",
      " 'model_name': 'gru',\n",
      " 'optimizer': 'adam',\n",
      " 'seed_value': 11,\n",
      " 'train_epochs': 200,\n",
      " 'train_model': True}\n",
      "\n",
      "\n",
      "{'asset_name': 'bigru_1e-6LearningRate_200Epochs_0.5Dropout',\n",
      " 'dataset_name': 'covid19-twitter',\n",
      " 'dataset_path': 'datasets/covid19-twitter/raw_dataset.pickle',\n",
      " 'dropout': 0.5,\n",
      " 'evaluate_model': True,\n",
      " 'fine_tune_word_embeddings': False,\n",
      " 'generate_explanation': True,\n",
      " 'generate_explanation_for_one_instance': False,\n",
      " 'hidden_units': 128,\n",
      " 'learning_rate': 1e-06,\n",
      " 'lime_no_of_samples': 1000,\n",
      " 'mini_batch_size': 50,\n",
      " 'model_name': 'bigru',\n",
      " 'optimizer': 'adam',\n",
      " 'seed_value': 11,\n",
      " 'train_epochs': 200,\n",
      " 'train_model': True}\n",
      "\n",
      "\n",
      "{'asset_name': 'transformer_1e-6LearningRate_200Epochs_0.5Dropout_2Heads_LimeExpFixed',\n",
      " 'dataset_name': 'covid19-twitter',\n",
      " 'dataset_path': 'datasets/covid19-twitter/raw_dataset.pickle',\n",
      " 'dropout': 0.5,\n",
      " 'evaluate_model': True,\n",
      " 'fine_tune_word_embeddings': False,\n",
      " 'generate_explanation': True,\n",
      " 'generate_explanation_for_one_instance': False,\n",
      " 'hidden_units': 2,\n",
      " 'learning_rate': 1e-06,\n",
      " 'lime_no_of_samples': 1000,\n",
      " 'mini_batch_size': 50,\n",
      " 'model_name': 'transformer',\n",
      " 'optimizer': 'adam',\n",
      " 'seed_value': 11,\n",
      " 'train_epochs': 200,\n",
      " 'train_model': True}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Identify the configuration parameters for every benchmark model\n",
    "import pickle\n",
    "import pprint\n",
    "\n",
    "FLAT_CLASSIFIERS = [\n",
    "                  \"mlp_1e-6LearningRate_200Epochs_0.5Dropout\",\n",
    "                  \"cnn_1e-6LearningRate_200Epochs_0.5Dropout\",\n",
    "                  \"lstm_1e-6LearningRate_200Epochs_0.5Dropout\",\n",
    "                  \"bilstm_1e-6LearningRate_200Epochs_0.5Dropout\",\n",
    "                  \"gru_1e-6LearningRate_200Epochs_0.5Dropout\",\n",
    "                  \"bigru_1e-6LearningRate_200Epochs_0.5Dropout\",\n",
    "                  \"transformer_1e-6LearningRate_200Epochs_0.5Dropout_2Heads_LimeExpFixed\"\n",
    "                 ]\n",
    "\n",
    "BASELINES = [\n",
    "            \"bertweet_mlp_DropoutAddedInMlp\",\n",
    "            \"bertweet_cnn\",\n",
    "            \"bertweet_lstm\",\n",
    "            \"bertweet_bilstm\",\n",
    "            \"bertweet_gru\",\n",
    "            \"bertweet_bigru\",\n",
    "            \"bertweet_transformer\",\n",
    "            \"gpt2_mlp\",\n",
    "            \"gpt2_cnn\",\n",
    "            \"gpt2_lstm\",\n",
    "            \"gpt2_bilstm\",\n",
    "            \"gpt2_gru\",\n",
    "            \"gpt2_bigru\",\n",
    "            \"gpt2_transformer\",\n",
    "            \"xlnet_mlp_20BatchSize\",\n",
    "            \"xlnet_cnn_20BatchSize\",\n",
    "            \"xlnet_lstm_20BatchSize\",\n",
    "            \"xlnet_bilstm_20BatchSize\",\n",
    "            \"xlnet_gru_20BatchSize\",\n",
    "            \"xlnet_bigru_20BatchSize\",\n",
    "            \"xlnet_transformer_20BatchSize\",\n",
    "            \"roberta_mlp_20BatchSize\",\n",
    "            \"roberta_cnn_20BatchSize\",\n",
    "            \"roberta_lstm_20BatchSize\",\n",
    "            \"roberta_bilstm_20BatchSize\",\n",
    "            \"roberta_gru_20BatchSize\",\n",
    "            \"roberta_bigru_20BatchSize\",\n",
    "            \"roberta_transformer_20BatchSize\",\n",
    "            \"distilbert_mlp_20BatchSize\",\n",
    "            \"distilbert_cnn_20BatchSize\",\n",
    "            \"distilbert_lstm_20BatchSize\",\n",
    "            \"distilbert_bilstm_20BatchSize\",\n",
    "            \"distilbert_gru_20BatchSize\",\n",
    "            \"distilbert_bigru_20BatchSize\",\n",
    "            \"distilbert_transformer_20BatchSize\",\n",
    "          ]\n",
    "\n",
    "for index, classifier in enumerate(FLAT_CLASSIFIERS):\n",
    "    with open(\"assets/configurations/\"+classifier+\".pickle\", \"rb\") as handle:\n",
    "        config = pickle.load(handle)\n",
    "    pprint.pprint(config)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53152f5c-40bf-4b1d-a255-393b6f425b51",
   "metadata": {},
   "source": [
    "#### Run the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d9b284-036b-4051-a83d-678f9a9d4469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify of configuration parameters of each benchmark model whose result need to reproduced \n",
    "##Run the previous code to identify the parameters of the desired model\n",
    "##If you would just like to check the existing trained models, set the \"train_model\" value to False\n",
    "config = {\n",
    "            \"asset_name\":\"bertweet_gru_reproducibility_test\",\n",
    "            \"model_name\":\"bertweet_gru\",\n",
    "            \"seed_value\":11,\n",
    "            \"dataset_name\":\"covid19-twitter\",\n",
    "            \"train_model\":False,\n",
    "            \"evaluate_model\":False,\n",
    "            \"generate_explanations\":False,\n",
    "            \"generate_explanation_for_one_instance\":True,\n",
    "            \"fine_tune_word_embeddings\":True,\n",
    "            \"optimizer\":\"adam\",\n",
    "            \"learning_rate\":1e-6,\n",
    "            \"mini_batch_size\":50,\n",
    "            \"train_epochs\":2,\n",
    "            \"dropout\":0.5,\n",
    "            \"lime_no_of_samples\":1000,\n",
    "            \"hidden_units\":128\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e9df2fb-4754-4cfa-9ff8-8a5526f47660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 12:00:54.472369: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-13 12:00:54.472405: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-13 12:00:54.472437: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-13 12:00:54.480405: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Num GPUs Available:  1\n",
      "\n",
      "Creating input data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 12:02:00.987743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9689 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word vectors created\n",
      "\n",
      "Converted 36063 words (27654 misses)\n",
      "\n",
      "Building model\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 81)]                 0         []                            \n",
      "                                                                                                  \n",
      " tf.cast (TFOpLambda)        (None, 81)                   0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " tf.math.not_equal (TFOpLam  (None, 81)                   0         ['tf.cast[0][0]']             \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.cast_1 (TFOpLambda)      (None, 81)                   0         ['tf.math.not_equal[0][0]']   \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobert  TFBaseModelOutputWithPooli   1348999   ['tf.cast[0][0]',             \n",
      " aModel)                     ngAndCrossAttentions(last_   68         'tf.cast_1[0][0]']           \n",
      "                             hidden_state=(None, 81, 76                                           \n",
      "                             8),                                                                  \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " classifier (GRU)            (None, 128)                  344832    ['tf_roberta_model[0][0]']    \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1)                    129       ['classifier[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 135244929 (515.92 MB)\n",
      "Trainable params: 135244929 (515.92 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import argparse\n",
    "import pprint\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "print(\"\\nNum GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "import argparse\n",
    "import subprocess as sp\n",
    "import distutils\n",
    "import pprint\n",
    "\n",
    "#change the code execution directory to current directory\n",
    "os.chdir(os.getcwd())\n",
    "\n",
    "#scripts\n",
    "from scripts.preprocess_dataset import Preprocess_dataset\n",
    "from scripts.word_vectors import Word_vectors\n",
    "from scripts.dataset_division import Dataset_division\n",
    "\n",
    "from scripts.train_mlp import train_mlp\n",
    "from scripts.train_transformer import train_transformer\n",
    "from scripts.train_cnn import train_cnn\n",
    "from scripts.train_rnn import train_rnn\n",
    "\n",
    "from scripts.train_bertweet_mlp import train_bertweet_mlp\n",
    "from scripts.train_bertweet_transformer import train_bertweet_transformer\n",
    "from scripts.train_bertweet_cnn import train_bertweet_cnn\n",
    "from scripts.train_bertweet_rnn import train_bertweet_rnn\n",
    "from scripts.train_gpt2_mlp import train_gpt2_mlp\n",
    "from scripts.train_gpt2_transformer import train_gpt2_transformer\n",
    "from scripts.train_gpt2_rnn import train_gpt2_rnn\n",
    "from scripts.train_gpt2_cnn import train_gpt2_cnn\n",
    "from scripts.train_roberta_mlp import train_roberta_mlp\n",
    "from scripts.train_roberta_cnn import train_roberta_cnn\n",
    "from scripts.train_roberta_transformer import train_roberta_transformer\n",
    "from scripts.train_roberta_rnn import train_roberta_rnn\n",
    "from scripts.train_xlnet_mlp import train_xlnet_mlp\n",
    "from scripts.train_xlnet_cnn import train_xlnet_cnn\n",
    "from scripts.train_xlnet_transformer import train_xlnet_transformer\n",
    "from scripts.train_xlnet_rnn import train_xlnet_rnn\n",
    "from scripts.train_distilbert_mlp import train_distilbert_mlp\n",
    "from scripts.train_distilbert_cnn import train_distilbert_cnn\n",
    "from scripts.train_distilbert_transformer import train_distilbert_transformer\n",
    "from scripts.train_distilbert_rnn import train_distilbert_rnn\n",
    "\n",
    "#disable warnings\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#set the gpu execution device with highest free memory\n",
    "def mask_unused_gpus(leave_unmasked=1): # No of avaialbe GPUs on the system\n",
    "    COMMAND = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    try:\n",
    "        _output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
    "        memory_free_info = _output_to_list(sp.check_output(COMMAND.split()))[1:]\n",
    "        memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "        available_gpus = [i for i, x in enumerate(memory_free_values)]\n",
    "        if len(available_gpus) < leave_unmasked: raise ValueError('Found only %d usable GPUs in the system' % len(available_gpus))\n",
    "        gpu_with_highest_free_memory = 0\n",
    "        highest_free_memory = 0\n",
    "        for index, memory in enumerate(memory_free_values):\n",
    "            if memory > highest_free_memory:\n",
    "                highest_free_memory = memory\n",
    "                gpu_with_highest_free_memory = index\n",
    "        return str(gpu_with_highest_free_memory)\n",
    "    except Exception as e:\n",
    "        print('\"nvidia-smi\" is probably not installed. GPUs are not masked', e)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = mask_unused_gpus()\n",
    "\n",
    "#prepare the dataset\n",
    "print(\"\\nCreating input data\")\n",
    "preprocessed_dataset = Preprocess_dataset(config).preprocess_covid_tweets(dataset)\n",
    "word_vectors, word_index = Word_vectors(config).create_word_vectors(preprocessed_dataset)\n",
    "train_dataset, val_datasets, test_datasets = Dataset_division(config).train_val_test_split(preprocessed_dataset)\n",
    "\n",
    "#create model\n",
    "print(\"\\nBuilding model\")\n",
    "\n",
    "#flat classifiers\n",
    "if config[\"model_name\"] == \"mlp\":\n",
    "    train_mlp(config).train_model(train_dataset, val_datasets, test_datasets, word_index, word_vectors)\n",
    "elif config[\"model_name\"] == \"transformer\":\n",
    "    train_transformer(config).train_model(train_dataset, val_datasets, test_datasets, word_index, word_vectors)\n",
    "elif config[\"model_name\"] == \"cnn\":\n",
    "    train_cnn(config).train_model(train_dataset, val_datasets, test_datasets, word_index, word_vectors)\n",
    "elif config[\"model_name\"] in [\"lstm\", \"bilstm\", \"gru\", \"bigru\"]:\n",
    "    train_rnn(config).train_model(train_dataset, val_datasets, test_datasets, word_index, word_vectors)\n",
    "\n",
    "#cwe classifiers\n",
    "elif config[\"model_name\"] == \"bertweet_mlp\":\n",
    "    train_bertweet_mlp(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"bertweet_transformer\":\n",
    "    train_bertweet_transformer(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"bertweet_cnn\":\n",
    "    train_bertweet_cnn(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] in [\"bertweet_lstm\", \"bertweet_bilstm\", \"bertweet_gru\", \"bertweet_bigru\"]:\n",
    "    train_bertweet_rnn(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"gpt2_mlp\":\n",
    "    train_gpt2_mlp(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"gpt2_transformer\":\n",
    "    train_gpt2_transformer(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] in [\"gpt2_lstm\", \"gpt2_bilstm\", \"gpt2_gru\", \"gpt2_bigru\"]:\n",
    "    train_gpt2_rnn(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"gpt2_cnn\":\n",
    "    train_gpt2_cnn(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"roberta_mlp\":\n",
    "    train_roberta_mlp(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"roberta_cnn\":\n",
    "    train_roberta_cnn(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"roberta_transformer\":\n",
    "    train_roberta_transformer(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] in [\"roberta_lstm\", \"roberta_bilstm\", \"roberta_gru\", \"roberta_bigru\"]:\n",
    "    train_roberta_rnn(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"xlnet_mlp\":\n",
    "    train_xlnet_mlp(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"xlnet_cnn\":\n",
    "    train_xlnet_cnn(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"xlnet_transformer\":\n",
    "    train_xlnet_transformer(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] in [\"xlnet_lstm\", \"xlnet_bilstm\", \"xlnet_gru\", \"xlnet_bigru\"]:\n",
    "    train_xlnet_rnn(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"distilbert_mlp\":\n",
    "    train_distilbert_mlp(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"distilbert_cnn\":\n",
    "    train_distilbert_cnn(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] == \"distilbert_transformer\":\n",
    "    train_distilbert_transformer(config).train_model(train_dataset, val_datasets, test_datasets)\n",
    "elif config[\"model_name\"] in [\"distilbert_lstm\", \"distilbert_bilstm\", \"distilbert_gru\", \"distilbert_bigru\"]:\n",
    "    train_distilbert_rnn(config).train_model(train_dataset, val_datasets, test_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47859a4-f474-41c2-8b7c-9a389cd0e744",
   "metadata": {},
   "source": [
    "### Option 2: Check the existing results presented in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d90b10f5-e5f4-45a0-95df-9cdd9c534904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 15:12:01.890356: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-13 15:12:01.890399: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-13 15:12:01.890438: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-13 15:12:01.897826: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "mlp sentiment accuracy:  0.834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "mlp PERCY score:  0.025\n",
      "\n",
      "\n",
      "cnn sentiment accuracy:  0.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cnn PERCY score:  0.07\n",
      "\n",
      "\n",
      "lstm sentiment accuracy:  0.845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "lstm PERCY score:  0.039\n",
      "\n",
      "\n",
      "bilstm sentiment accuracy:  0.871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bilstm PERCY score:  0.07\n",
      "\n",
      "\n",
      "gru sentiment accuracy:  0.867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "gru PERCY score:  0.065\n",
      "\n",
      "\n",
      "bigru sentiment accuracy:  0.872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bigru PERCY score:  0.07\n",
      "\n",
      "\n",
      "transformer sentiment accuracy:  0.889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "transformer PERCY score:  0.059\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from scripts.corrects_distribution import Corrects_distribution\n",
    "from scripts.percy_score import calculate_percy\n",
    "\n",
    "FLAT_CLASSIFIERS = [\n",
    "                  \"mlp_1e-6LearningRate_200Epochs_0.5Dropout\",\n",
    "                  \"cnn_1e-6LearningRate_200Epochs_0.5Dropout\",\n",
    "                  \"lstm_1e-6LearningRate_200Epochs_0.5Dropout\",\n",
    "                  \"bilstm_1e-6LearningRate_200Epochs_0.5Dropout\",\n",
    "                  \"gru_1e-6LearningRate_200Epochs_0.5Dropout\",\n",
    "                  \"bigru_1e-6LearningRate_200Epochs_0.5Dropout\",\n",
    "                  \"transformer_1e-6LearningRate_200Epochs_0.5Dropout_2Heads_LimeExpFixed\"\n",
    "                 ]\n",
    "\n",
    "CWE = [\n",
    "        \"bertweet_mlp_DropoutAddedInMlp\",\n",
    "        \"bertweet_cnn\",\n",
    "        \"bertweet_lstm\",\n",
    "        \"bertweet_bilstm\",\n",
    "        \"bertweet_gru\",\n",
    "        \"bertweet_bigru\",\n",
    "        \"bertweet_transformer\",\n",
    "        \"gpt2_mlp\",\n",
    "        \"gpt2_cnn\",\n",
    "        \"gpt2_lstm\",\n",
    "        \"gpt2_bilstm\",\n",
    "        \"gpt2_gru\",\n",
    "        \"gpt2_bigru\",\n",
    "        \"gpt2_transformer\",\n",
    "        \"xlnet_mlp_20BatchSize\",\n",
    "        \"xlnet_cnn_20BatchSize\",\n",
    "        \"xlnet_lstm_20BatchSize\",\n",
    "        \"xlnet_bilstm_20BatchSize\",\n",
    "        \"xlnet_gru_20BatchSize\",\n",
    "        \"xlnet_bigru_20BatchSize\",\n",
    "        \"xlnet_transformer_20BatchSize\",\n",
    "        \"roberta_mlp_20BatchSize\",\n",
    "        \"roberta_cnn_20BatchSize\",\n",
    "        \"roberta_lstm_20BatchSize\",\n",
    "        \"roberta_bilstm_20BatchSize\",\n",
    "        \"roberta_gru_20BatchSize\",\n",
    "        \"roberta_bigru_20BatchSize\",\n",
    "        \"roberta_transformer_20BatchSize\",\n",
    "        \"distilbert_mlp_20BatchSize\",\n",
    "        \"distilbert_cnn_20BatchSize\",\n",
    "        \"distilbert_lstm_20BatchSize\",\n",
    "        \"distilbert_bilstm_20BatchSize\",\n",
    "        \"distilbert_gru_20BatchSize\",\n",
    "        \"distilbert_bigru_20BatchSize\",\n",
    "        \"distilbert_transformer_20BatchSize\",\n",
    "      ]\n",
    "\n",
    "for index, classifier in enumerate(FLAT_CLASSIFIERS):\n",
    "    with open(\"assets/configurations/\"+classifier+\".pickle\", \"rb\") as handle:\n",
    "        configuration = pickle.load(handle)\n",
    "    model_name = configuration[\"model_name\"]\n",
    "    \n",
    "    with open(\"assets/results/\"+classifier+\".pickle\", \"rb\") as handle:\n",
    "        results = pickle.load(handle)\n",
    "        results = pd.DataFrame(results)\n",
    "    one_rule = pd.concat([results.loc[(results[\"rule_label\"]!=0)&(results[\"contrast\"]==1)], results.loc[(results[\"rule_label\"]!=0)&(results[\"contrast\"]==0)]])\n",
    "    one_rule = one_rule.reset_index(drop=True)\n",
    "    one_rule_contrast_pos = one_rule.loc[(one_rule[\"contrast\"]==1)&(one_rule[\"sentiment_label\"]==1)]\n",
    "    one_rule_contrast_neg = one_rule.loc[(one_rule[\"contrast\"]==1)&(one_rule[\"sentiment_label\"]==0)]\n",
    "    one_rule_contrast_pos_sample = one_rule_contrast_pos.sample(n=1350, random_state=11)\n",
    "    one_rule_contrast_neg_sample = one_rule_contrast_neg.sample(n=1351, random_state=11)\n",
    "    one_rule.drop(one_rule_contrast_pos_sample.index, inplace = True)\n",
    "    one_rule.drop(one_rule_contrast_neg_sample.index, inplace = True)\n",
    "    results = pd.concat([results.loc[results[\"rule_label\"]==0], one_rule])\n",
    "    results = results.reset_index(drop=True)\n",
    "    base_sent_corrects = Corrects_distribution(len(results['tweet_id'])).model_sentiment_correct_distributions(results)\n",
    "    accuracy = sum(base_sent_corrects[\"one_rule\"])/len(base_sent_corrects[\"one_rule\"])\n",
    "    print(\"\\n\")\n",
    "    print(model_name+\" sentiment accuracy: \", round(accuracy, 3))\n",
    "    \n",
    "    with open(\"assets/lime_explanations/\"+classifier+\".pickle\", \"rb\") as handle:\n",
    "        explanations = pickle.load(handle)\n",
    "        explanations = pd.DataFrame(explanations)\n",
    "    explanations.drop(one_rule_contrast_pos_sample.index, inplace = True)\n",
    "    explanations.drop(one_rule_contrast_neg_sample.index, inplace = True)\n",
    "    explanations = explanations.reset_index(drop=True)\n",
    "    one_rule_results = pd.concat([results.loc[(results[\"rule_label\"]!=0)&(results[\"contrast\"]==1)], \n",
    "                                  results.loc[(results[\"rule_label\"]!=0)&(results[\"contrast\"]==0)]])\n",
    "    percy = calculate_percy(one_rule_results, explanations)\n",
    "    percy_value = sum(percy[\"one_rule\"])/len(percy[\"one_rule\"])\n",
    "    print(\"\\n\")\n",
    "    print(model_name+\" PERCY score: \", round(percy_value, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
